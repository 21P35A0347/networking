server stacks in monolithic process are called as working nodes in kubernates

-> we have to install kubernates in every single working node

->the main one is master node where the nodes connected to master node are working node

->this master node is connected to all the worker nodes and decides where to host our applications(docker containers) and how to piece them together even managing and orchestratiog them  starting ,stopping,updates that kind of thing

-> 3 major adv k8s  provides;
	->deployment
	->development
	->monitoring

lets assume there is a simple cloud native application
->lets say the front of that applcation is something that we wrote in react  and backend by nodejs
->lets say that the database acess application is with java
->for accessing external apis we use python flask
->here the .js and java and .py  servers  are worker nodes in k8s where k8s installed in every node connected to master node

->3 main advantages deployment,making development easier and development and monitoring tools

->deployment coming back to our application architecture(.js,.py.java) 
->lets say we want to deploy  that react(.js) application about 8 times then we say 8 instances eache of them we expect to consume about 128mb and we actually specify other policies as well like when to reatart,
->a kubernates deployment is not a one timething its something that grow and lives and breathes with our application and full stack for eg if the react app happens to crash kubenetes will automatically restart it to get back to that state that we identified when we first created deployment
->we can additionaly say that its made deployment in addition to scalling is easier

->development we have deployed all the microservices(.js,.py,.java) now development
->we have lots of different microservices out there with different endpoints
->for example our frontend(.js) needs to access database(.js) there may 8 diff versions that talk to that base database we have to talk with one of them to get our request fullfilled 
->what kubernates does is deploys load balancer for all the microservices and in advantage it takes service registry and discovery capabilites to allow our applications talk to each other using something 
called as kubernetes service so for each of these k8s  will also create a service
->lets say service a,service b,service c

->these applications can speak to each other just by using these service names that are layed in kubernates essentially we can say kubernates has made development easier

->monotoring : kubernetes has a lot of built in capapbilites which allows u to see logs,see cpu load, all in their neat UI but the fact is that there is sometimes more that you want to see with your application and the open source community out has developes a number of amazing tools to give u intorspection into your running application


KUBERNATES VS DOCKER :
	its not definitely not a choice of using one or the other its one of those things where kubernetes
	to take advantage of your existing docker workloads and run them at scale -tackle real complexities


"--driver=docker": This option specifies the container runtime to use for the Kubernetes cluster. By default, Minikube uses VirtualBox to create a virtual machine and runs Kubernetes on that. However, this option tells Minikube to use Docker as the container runtime instead. This means that Minikube will create a single-node Kubernetes cluster running on a Docker container on your local machine.


DEPLOYING IN MINIKUBE:
	download minikube and kubectl 
	open cmd and run "minkube start --driver=docker"
	next open cmd in yaml files path 
	do "kubectl apply -f frontend.yaml" (yaml file with both deployment and services combined together)
	then to view services do kubectl get services
	to run or view a service "minikube service (give servicename after seeing in kubectl get services)"
	clicke ctrl c to exit after
	USE "kubectl get namespace" to see namesapce
	to create namespace "kubectl create namespace [name of namespace]" 
	
	->to see kubernates dashboard use "minikube dashboard"

NAME SPACE TOPIC:
		In Kubernetes, a namespace is a logical partition within a cluster that allows you to isolate and organize your resources. Namespaces provide a way to divide cluster resources 		between multiple teams or projects, while still sharing a single cluster.

		Each namespace has its own set of Kubernetes objects, such as pods, services, and replication controllers, that are named uniquely within the namespace. By default, Kubernetes 		resources are created in the "default" namespace if no other namespace is specified.

		Using namespaces can help with security, resource allocation, and collaboration within a Kubernetes cluster. For example, you can use namespaces to restrict access to certain 		resources to specific teams or individuals, or to limit the amount of resources that a particular application can consume.

		Kubernetes also provides the ability to create multiple namespaces for different environments such as development, testing, and production to prevent conflicts between resources.

KUBERNATES MESH:
	MESH in sense creating a communication between 2 microservices